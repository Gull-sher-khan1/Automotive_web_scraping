{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJNvdKviVuLT"
      },
      "outputs": [],
      "source": [
        "#Install and Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Ensure necessary NLTK data files downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/sample_data/AWS_Autonews_data.csv\")\n",
        "\n",
        "\n",
        "# Data Overview\n",
        "print(\"Data Overview:\")\n",
        "print(data.head())\n",
        "print(data.shape)\n",
        "print(data.dtypes)\n",
        "\n",
        "# 2. Identify missing values before the fix\n",
        "print(\"\\nMissing Values Before Fix:\")\n",
        "missing_values_before = data.isnull().sum()\n",
        "print(missing_values_before)\n",
        "\n",
        "# 3. Fix missing values\n",
        "data['body'].fillna('', inplace=True)\n",
        "data['image_link'].fillna('No image available', inplace=True)\n",
        "\n",
        "# 4. Identify missing values after the fix\n",
        "print(\"\\nMissing Values After Fix:\")\n",
        "missing_values_after = data.isnull().sum()\n",
        "print(missing_values_after)\n",
        "\n",
        "# Clean and Correct `publish_date` Column\n",
        "def parse_dates(date_str):\n",
        "    if pd.isna(date_str):  # Handle NaT\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.strptime(date_str, '%A, %B %d, %Y')\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "    try:\n",
        "        return pd.to_datetime(date_str, errors='coerce', dayfirst=True)\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "data['parsed_publish_date'] = data['publish_date'].apply(parse_dates)\n",
        "data = data.dropna(subset=['parsed_publish_date'])\n",
        "\n",
        "# Analyze word count in titles\n",
        "data['title_word_count'] = data['heading'].apply(lambda x: len(str(x).split()))\n",
        "print(\"\\nWord Count Summary:\\n\", data['title_word_count'].describe())\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(data['title_word_count'], bins=15, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Word Count in Titles')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Define categories and keywords\n",
        "categories = {\n",
        "    'Vehicle Maintenance': ['maintenance', 'repair', 'service', 'oil change', 'tire rotation', 'garage'],\n",
        "    'Auto Parts': ['parts', 'spare', 'brake', 'engine', 'transmission'],\n",
        "    'Technology and Innovation': ['technology', 'innovation', 'ai', 'artificial intelligence', 'machine learning'],\n",
        "    'Market and industry Trend': ['industry', 'news', 'market', 'trend', 'report'],\n",
        "    'Training and Education': ['training', 'education', 'course', 'certification', 'workshop'],\n",
        "    'Electric Vehicles': ['electric vehicle', 'ev', 'battery', 'charging', 'tesla'],\n",
        "    'Autonomous Vehicles': ['autonomous', 'self-driving', 'driverless', 'robotics', 'ai'],\n",
        "    'Recalls and Safety': ['recall', 'safety', 'accident', 'defect', 'warning'],\n",
        "    'Customer and Dealership Service': ['customer service', 'dealership', 'service', 'experience', 'feedback'],\n",
        "    'Events and Trade Shows': ['event', 'trade show', 'conference', 'expo', 'summit'],\n",
        "    'Design and Engineering': ['design', 'engineering', 'aerodynamics', 'architecture', 'prototype'],\n",
        "    'Marketing and Sales': ['marketing', 'sales', 'promotion', 'campaign', 'advertising'],\n",
        "    'Brands and Manufacturers': ['brand', 'manufacturer', 'company', 'ford', 'gm'],\n",
        "    'Vehicle Performance and Tuning': ['performance', 'tuning', 'horsepower', 'speed', 'dyno'],\n",
        "    'Job': ['job', 'career', 'employment', 'hiring', 'vacancy'],\n",
        "    'Insurance': ['insurance', 'policy', 'coverage', 'claim', 'premium'],\n",
        "    'Manufacturing': ['manufacturing', 'production', 'factory', 'assembly', 'plant'],\n",
        "    'Mot': ['mot', 'test', 'inspection', 'certificate', 'roadworthy'],\n",
        "    'Customer Experience': ['customer experience', 'satisfaction', 'loyalty', 'service', 'feedback'],\n",
        "    'Resale': ['resale', 'aftermarket', 'auction', 'resell', 'exchange']\n",
        "}\n",
        "\n",
        "# Text Preprocessing\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "data['processed_body'] = data['body'].apply(preprocess_text)\n",
        "\n",
        "# Categorize text based on keywords\n",
        "def categorize_text(text, categories):\n",
        "    text_set = set(text)\n",
        "    category_scores = {}\n",
        "\n",
        "    for category, keywords in categories.items():\n",
        "        keyword_set = set(keywords)\n",
        "        matches = text_set.intersection(keyword_set)\n",
        "        category_scores[category] = len(matches)\n",
        "\n",
        "    if category_scores:\n",
        "        return max(category_scores, key=category_scores.get)\n",
        "    else:\n",
        "        return 'Uncategorized'\n",
        "\n",
        "data['category'] = data['processed_body'].apply(lambda x: categorize_text(x, categories))\n",
        "\n",
        "# Output a table to show the first 10 and last 10 categories and subcategories\n",
        "category_table = data[['category', 'processed_body']].groupby('category').count().reset_index()\n",
        "\n",
        "# Calculate the percentage of each category\n",
        "# Calculate the percentage of each category and round to 10 decimal places\n",
        "total_count = category_table['processed_body'].sum()\n",
        "category_table['percentage'] = (category_table['processed_body'] / total_count * 100).round(1)\n",
        "\n",
        "# Output the first 10 and last 10 categories and subcategories along with their percentages\n",
        "print(\"First 10 Categories and Subcategories:\")\n",
        "print(category_table.head(10))\n",
        "print(\"\\nLast 10 Categories and Subcategories:\")\n",
        "print(category_table.tail(10))\n",
        "\n",
        "\n",
        "# Topic Distribution Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data['category'], order=data['category'].value_counts().index)\n",
        "plt.title('Distribution of News Articles Across Categories')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 8. Wordcloud for relevant keywords for filtering, including car brands\n",
        "\n",
        "relevant_keywords = {\n",
        "    \"Electric Vehicles\": ['electric vehicle', 'ev', 'battery', 'charging', 'tesla'],\n",
        "    \"Technology & Innovation\": [\"technology\", \"innovation\", \"ai\", \"software\", \"digital\", \"automation\",\"machine learning\",\"artificial intelligence\"],\n",
        "    \"Auto Parts\": [\"parts\", \"spare\", \"brake\", \"engine\", \"transmission\"],\n",
        "    \"Industry News\": [\"industry\", \"news\", \"market\", \"trend\", \"report\"],\n",
        "    \"Brands & Manufacturers\": [\n",
        "        \"brand\", \"manufacturer\", \"company\", \"production\", \"industry\", \"market\",\n",
        "        \"toyota\", \"ford\", \"volvo\", \"kia\", \"honda\", \"bmw\", \"mercedes\", \"nissan\",\n",
        "        \"hyundai\", \"audi\", \"tesla\", \"chevrolet\", \"jeep\", \"mazda\", \"subaru\", \"volkswagen\"\n",
        "    ],\n",
        "    \"MOT\": [\"mot\", \"inspection\", \"test\", \"certification\"],\n",
        "    \"Sales\": [\"sales\", \"market\", \"dealership\", \"retail\", \"customer\", \"price\"],\n",
        "    \"Vehicle Maintenance\": [\"maintenance\", \"service\", \"repair\", \"checkup\", \"diagnosis\", \"mechanic\"],\n",
        "    \"Design & Engineering\": [\"design\", \"engineering\", \"aerodynamics\", \"materials\", \"manufacturing\"],\n",
        "    \"Vehicle Performance\": [\"performance\", \"acceleration\", \"speed\", \"efficiency\", \"power\", \"handling\"],\n",
        "    \"Education & Training\": [\"education\", \"training\", \"learning\", \"courses\", \"certification\"],\n",
        "    \"Autonomous Vehicles\": [\"autonomous\", \"self-driving\", \"automation\", \"sensor\", \"lidar\", \"ai\"],\n",
        "    \"Repair & Diagnosis\": [\"repair\", \"diagnosis\", \"fix\", \"mechanic\", \"maintenance\"],\n",
        "    \"Organization\": [\"organization\", \"company\", \"management\", \"structure\", \"leadership\"]\n",
        "}\n",
        "\n",
        "def filter_relevant_words(text, relevant_keywords):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if any(word in keyword_list for keyword_list in relevant_keywords.values())]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Combine all relevant words from all categories\n",
        "all_text = ' '.join([' '.join(words) for words in data['processed_body']])\n",
        "\n",
        "# Filter the combined text using the relevant keywords\n",
        "filtered_text = filter_relevant_words(all_text, relevant_keywords)\n",
        "\n",
        "# Generate a word cloud for the filtered text\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(filtered_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud for All Relevant Categories')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Distribution of News Articles Across Categories\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data['category'], order=data['category'].value_counts().index)\n",
        "plt.title('Distribution of News Articles Across Categories')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Pie chart for target variable (category)\n",
        "plt.figure(figsize=(12, 6))\n",
        "category_counts = data['category'].value_counts()\n",
        "plt.pie(category_counts, labels=None, autopct=None, startangle=140, colors=sns.color_palette(\"tab10\"))\n",
        "plt.title('Distribution of News Categories')\n",
        "plt.axis('equal')\n",
        "legend_labels = [f\"{category}: {count} ({percentage:.1f}%)\" for category, count, percentage in zip(category_counts.index, category_counts, 100*category_counts/category_counts.sum())]\n",
        "plt.legend(legend_labels, loc='center right', bbox_to_anchor=(1.3, 0.6), fancybox=True, shadow=True)\n",
        "plt.show()\n",
        "\n",
        "# Temporal Trends Analysis\n",
        "data['year_month'] = pd.to_datetime(data['parsed_publish_date'])\n",
        "top_two_categories = data['category'].value_counts().index[:2]\n",
        "temporal_data = data[data['category'].isin(top_two_categories)]\n",
        "temporal_trends = temporal_data.groupby([temporal_data['year_month'].dt.to_period(\"M\"), 'category']).size().unstack(fill_value=0)\n",
        "temporal_trends.plot(kind='line', figsize=(14, 7), title='Temporal Trends of Top 2 Categories')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.show()\n",
        "\n",
        "# Yearly Trends for Selected Categories\n",
        "data['year'] = data['parsed_publish_date'].dt.year\n",
        "categories_to_analyze = ['Auto Parts', 'Vehicle Maintenance']\n",
        "filtered_data = data[data['category'].isin(categories_to_analyze)]\n",
        "yearly_trends = filtered_data.groupby(['year', 'category']).size().unstack(fill_value=0)\n",
        "yearly_trends.plot(kind='line', figsize=(14, 7), marker='o', title='Yearly Trends for Auto Parts and Vehicle Maintenance')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Assuming 'processed_body' is the column with the cleaned and tokenized text data\n",
        "data['bigrams'] = data['processed_body'].apply(lambda x: get_ngrams(x, 2))\n",
        "data['trigrams'] = data['processed_body'].apply(lambda x: get_ngrams(x, 3))\n",
        "\n",
        "# Flatten the list of bigrams and trigrams and count the most common ones\n",
        "top_bigrams = Counter([item for sublist in data['bigrams'] for item in sublist]).most_common(10)\n",
        "top_trigrams = Counter([item for sublist in data['trigrams'] for item in sublist]).most_common(10)\n",
        "\n",
        "# Convert the top bigrams and trigrams to DataFrames for better display\n",
        "bigram_df = pd.DataFrame(top_bigrams, columns=['Bigram', 'Count'])\n",
        "trigram_df = pd.DataFrame(top_trigrams, columns=['Trigram', 'Count'])\n",
        "\n",
        "# Display the tables\n",
        "print(\"Top Bigrams:\")\n",
        "print(bigram_df)\n",
        "\n",
        "print(\"\\nTop Trigrams:\")\n",
        "print(trigram_df)\n",
        "\n",
        "\n",
        "\n",
        "# Search for keywords and Tags within graphical interface\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# 9. Function to search for keywords with graphical interface\n",
        "def search_articles(keyword, data):\n",
        "    keyword = keyword.lower()\n",
        "    return data[data['body'].str.contains(keyword, na=False)]\n",
        "\n",
        "# Function to display search results\n",
        "def display_search_results(keyword):\n",
        "    results = search_articles(keyword, data)\n",
        "    if not results.empty:\n",
        "        print(f\"Found {len(results)} articles containing the keyword '{keyword}':\")\n",
        "        # Display available columns\n",
        "        display(results[['category', 'body']].head())  # Display top 5 results\n",
        "    else:\n",
        "        print(f\"No articles found containing the keyword '{keyword}'.\")\n",
        "\n",
        "# Create a search box widget\n",
        "search_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your search keyword here',\n",
        "    description='Search Tags and Keywords:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Function to handle search box input on pressing Enter\n",
        "def on_search_submit(change):\n",
        "    display_search_results(change.value)\n",
        "\n",
        "# Attach the function to handle submission (Enter key press)\n",
        "search_box.on_submit(on_search_submit)\n",
        "\n",
        "# Display the search box\n",
        "display(search_box)\n",
        "\n",
        "\n",
        "# NLP News Similarities\n",
        "\n",
        "# Jaccard Similarity\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "similarity_matrix = pd.DataFrame(index=data['category'].unique(), columns=data['category'].unique(), dtype=float)\n",
        "\n",
        "for cat1 in data['category'].unique():\n",
        "    for cat2 in data['category'].unique():\n",
        "        if cat1 != cat2:\n",
        "            # Flatten the list of words before joining into a string\n",
        "            set1 = set(' '.join([' '.join(words) for words in data[data['category'] == cat1]['processed_body']]).split())\n",
        "            set2 = set(' '.join([' '.join(words) for words in data[data['category'] == cat2]['processed_body']]).split())\n",
        "            similarity_matrix.at[cat1, cat2] = jaccard_similarity(set1, set2)\n",
        "\n",
        "plt.figure(figsize=(14, 12))  # Enlarge the plot\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap=\"YlGnBu\", linewidths=0.5, vmin=0, vmax=1)\n",
        "plt.title('Jaccard Similarity Between Categories Based on Article Bodies')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Cosine Similarity using TF-IDF\n",
        "corpus = [' '.join([' '.join(words) for words in data[data['category'] == category]['processed_body']]) for category in data['category'].unique()]\n",
        "\n",
        "vectorizer = TfidfVectorizer().fit_transform(corpus)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "cosine_sim_matrix = cosine_similarity(vectors)\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=data['category'].unique(), columns=data['category'].unique())\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cosine_sim_df, annot=True, cmap=\"YlGnBu\", linewidths=0.5, vmin=0, vmax=1)\n",
        "plt.title('Cosine Similarity Between Categories Based on Article Bodies')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# TF-IDF Weighted Word2Vec Similarity Between Categories Based on Article Bodies\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list of words in 'processed_body' to a single string for each row\n",
        "data['processed_body_str'] = data['processed_body'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Train a TF-IDF model on the entire corpus\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf.fit_transform(data['processed_body_str'])\n",
        "\n",
        "# Compute TF-IDF weighted word vectors for each category\n",
        "category_vectors = {}\n",
        "for category in data['category'].unique():\n",
        "    category_docs = data[data['category'] == category]['processed_body_str']\n",
        "    category_tfidf_matrix = tfidf.transform(category_docs)\n",
        "\n",
        "    # Calculate the weighted average of the word vectors\n",
        "    word_vectors = np.zeros((len(category_docs), tfidf_matrix.shape[1]))\n",
        "    for i, doc in enumerate(category_docs):\n",
        "        tfidf_weights = category_tfidf_matrix[i].toarray()\n",
        "        word_vectors[i] = tfidf_weights\n",
        "\n",
        "    category_vectors[category] = np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Compute similarity matrix\n",
        "category_names = list(category_vectors.keys())\n",
        "category_matrix = np.array([category_vectors[category] for category in category_names])\n",
        "tfidf_word2vec_sim_matrix = cosine_similarity(category_matrix)\n",
        "tfidf_word2vec_sim_df = pd.DataFrame(tfidf_word2vec_sim_matrix, index=category_names, columns=category_names)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(14, 12))  # Enlarge the plot\n",
        "sns.heatmap(tfidf_word2vec_sim_df, annot=True, cmap=\"YlGnBu\", linewidths=0.5, vmin=0, vmax=1)\n",
        "plt.title('TF-IDF Weighted Word2Vec Similarity Between Categories Based on Article Bodies')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Features Extraction and Models\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Convert the list of words in 'processed_body' to a single string for each row\n",
        "data['processed_body_str'] = data['processed_body'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Create the TF-IDF matrix\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(data['processed_body_str'])\n",
        "\n",
        "# Train a Word2Vec model on the tokenized text\n",
        "sentences = data['processed_body'].tolist()\n",
        "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create a function to compute the average Word2Vec vector for each document\n",
        "def document_vector(doc):\n",
        "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
        "    return np.mean(word2vec_model.wv[doc], axis=0) if doc else np.zeros(100)\n",
        "\n",
        "# Apply the function to each document in 'processed_body' to get the word embeddings\n",
        "data['word_embeddings'] = data['processed_body'].apply(document_vector)\n",
        "\n",
        "# Convert the embeddings into a 2D array\n",
        "word_embeddings = np.array(data['word_embeddings'].tolist())\n",
        "\n",
        "# Encode the categories into numeric labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['category_label'] = label_encoder.fit_transform(data['category'])\n",
        "\n",
        "# Define target\n",
        "y = data['category_label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train_word2vec, X_test_word2vec = train_test_split(word_embeddings, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Machine Learning and Hybrid Model Results\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVC': SVC(random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate models with each feature\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for feature_name, (X_train, X_test) in {\n",
        "        'TF-IDF': (X_train_tfidf, X_test_tfidf),\n",
        "        'Word2Vec': (X_train_word2vec, X_test_word2vec)\n",
        "    }.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        results[f'{model_name}_{feature_name}'] = {'accuracy': acc, 'f1_score': f1, 'confusion_matrix': cm}\n",
        "\n",
        "# Visualization of results\n",
        "df_results = pd.DataFrame(results).T.reset_index()\n",
        "df_results[['model', 'feature']] = df_results['index'].str.split('_', expand=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='model', y='accuracy', hue='feature', data=df_results)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='model', y='f1_score', hue='feature', data=df_results)\n",
        "plt.title('Model F1 Score Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrices for the best models\n",
        "best_models = sorted(results.items(), key=lambda item: item[1]['f1_score'], reverse=True)[:3]\n",
        "\n",
        "for model, metrics in best_models:\n",
        "    cm = metrics['confusion_matrix']\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "    plt.title(f'Confusion Matrix for {model}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Best Models by F1 Score:\")\n",
        "for model, metrics in best_models:\n",
        "    print(f\"{model}: F1 Score = {metrics['f1_score']}, Accuracy = {metrics['accuracy']}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}